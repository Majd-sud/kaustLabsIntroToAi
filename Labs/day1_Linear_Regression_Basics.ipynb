{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xp0xEaxQ9IdQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GMpV5X0P9RJ-"
      },
      "outputs": [],
      "source": [
        "# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n",
        "\n",
        "%pip install numpy\n",
        "%pip install scikit-learn\n",
        "%pip install matplotlib\n",
        "######\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-h6xg45AYUmF"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sklearn has implementations of multiple types of models. We'll be using LinearRegression API in it\n",
        "# For documentation on LinearRegression, visit here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bi0tehXm-p"
      },
      "source": [
        "#Contents:\n",
        "\n",
        "1. Implement 1 polynomial degree Linear Regression model from scratch (using numpy)\n",
        "2. Implement the same model using sklearn\n",
        "3. Take a complex function and try fitting a multi-polynomial degree Linear regression model on it.\n",
        "\n",
        "\n",
        "You need to know:\n",
        "\n",
        "1. **numpy** (for impelementation)\n",
        "2. a little bit of **matplotlib** (for visualization)\n",
        "\n",
        "\n",
        "Good to have knowledge of:\n",
        "\n",
        "1. Sklearn (details of the functions is given anyways)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shzta92y9oZI"
      },
      "source": [
        "## Implementing Linear Regression from Scratch (Using numpy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "PwIENVuG99b8",
        "outputId": "132d19b0-e4ca-45be-fce6-aee8f48a6bf6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-bc2a86aae209>, line 17)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-bc2a86aae209>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    y_points = ??\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Let's make some custom points (which would act as our dataset)\n",
        "# starting with a function with highest polynomial degree of 1\n",
        "\n",
        "# y = w0 + w1*x1\n",
        "\n",
        "w0 = 12  # bias\n",
        "w1 = 15  # first degree co-efficient\n",
        "noise_scale_factor = 5  # the higher this is, the rougher and farther the noisy output is from the best fit\n",
        "num_points = 50  # number of samples in data\n",
        "\n",
        "x_points = np.linspace(0, 1, num_points)  # num_points points from 0 to 1\n",
        "y_points_no_noise = w0 + w1*x_points  # The actual features which we'd feed to model will have slight noise\n",
        "noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)  # Look at the explanation below to see how this is calculated\n",
        "\n",
        "#############################\n",
        "# TODO, add noise\n",
        "y_points = ??\n",
        "#############################\n",
        "\n",
        "plt.plot(x_points, y_points_no_noise, label='No noise output (what we want as best fit)')\n",
        "plt.plot(x_points, y_points, label='Noisy output (labels fed to model)')\n",
        "_, ylim_top = plt.ylim()\n",
        "plt.ylim(0, ylim_top)  # 0 bottom makes it easy to visualize bias term\n",
        "\n",
        "plt.xlabel('X-points (features)')\n",
        "plt.ylabel('Y-points (output / labels)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5BL-GJbE-jm"
      },
      "source": [
        "### Explanation:\n",
        "\n",
        "```python\n",
        "noise_scale_factor = 5\n",
        "noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)\n",
        "y_points = y_points_no_noise + noise\n",
        "```\n",
        "\n",
        "- np.random.rand(*y_points_no_noise) generates an array of random values in range [0, 1) of the same shape as y_points_no_noise\n",
        "- we subtract 0.5 from it to bring it to range [-0.5, 0.5) (so there are also some values below the line after noise is added)\n",
        "- it might be possible that the values of w0, w1 or y_points is big enough for a noise point of [-0.5, 0.5) to not make much dent (for example, if a label is 1000, changing it to 1000.5 or 999.5 doesn't make much different) so we scale it with a scaling factor\n",
        "- The calculated noise is added to noise-less y_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9jdmjvX_ept"
      },
      "outputs": [],
      "source": [
        "# Custom calculating best fit\n",
        "\n",
        "num_bases = 1  # number of coefficient to polynomial terms (not including bias). We'll pick 1 since our original function had highest order term of 1\n",
        "\n",
        "A = [x_points**i for i in range(num_bases+1)]  # +1 in num_bases for bias\n",
        "A = np.array(A).T  # (m, n) matrix where m is number of samples and n is number of params (including bias). In our case, n is 2\n",
        "\n",
        "\n",
        "# Solve for w to find our params\n",
        "# Upper case: matrices. Lower case: vectors\n",
        "\n",
        "#  Aw = y\n",
        "#  Aᵀ(Aw) = Aᵀy  =>  AᵀAw = Aᵀy\n",
        "#  w = (AᵀA)⁻¹Aᵀy\n",
        "\n",
        "#############################\n",
        "# TODO, calcuate the w (param)\n",
        "w = ??\n",
        "#############################\n",
        "y_pred = A @ w  # calculate predictions using our optimized coefficient params and bias\n",
        "\n",
        "\n",
        "# Visualize the results\n",
        "\n",
        "#############################\n",
        "# TODO, plot the data with no noise\n",
        "plt.plot(x_points, ?? , label='data points without the noise (our goal)')\n",
        "#############################\n",
        "\n",
        "plt.plot(x_points, y_points, 'x', label='points in data (with noise)')\n",
        "plt.plot(x_points, y_pred, label='prediction')\n",
        "\n",
        "_, ylim_top = plt.ylim()\n",
        "plt.ylim(0, ylim_top+10)  # 0 bottom makes it easy to visualize bias term (intercept)\n",
        "\n",
        "plt.xlabel('X-points (features)')\n",
        "plt.ylabel('Y-points (output / labels)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsSuO-qfx5gB"
      },
      "outputs": [],
      "source": [
        "print(f'Actual weight values we used: {(w0, w1)}')\n",
        "print(f'Values calculated by Model: {tuple(w)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMgBfr0XmaA"
      },
      "source": [
        "## Let's now use sklearn to do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6yN5mOahfDz"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression(fit_intercept=True)\n",
        "model = model.fit(x_points.reshape(-1, 1), y_points)  # fit is used to train the model on the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds0xF__V2A_b"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "**fit_intercept=True**: This is used to tell the model that we also expect to calculate the intercept.\n",
        "For cases where the data is centralized during pre-processing, this value can be False since the intercept in that case is 0.\n",
        "But since that's not the case with us, we want the model to calculate it\n",
        "\n",
        "**x_points.reshape(-1, 1)**: Sklearn expects input features to be of the shape (m, n) where m is the number of samples and n is the number of features.\n",
        "Since the array we generated is of the shape (n,), we need to reshape it to (n, 1). This does not change the values, only the shape. Sklearn throws an error if we don't do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fU8fYejyYk7"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#TODO, predict the output\n",
        "y_pred = ??                    # Predict is used to prediction labels/outputs from feature inputs.\n",
        "#############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNu0XUo04t-a"
      },
      "source": [
        "Note: Normally it's a good idea to predict a separate set of data not seen during training (validation data), but here we're just looking at the basic implementation using sklearn so training data would also do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p-tfpPhznBX"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_points, y_points_no_noise, label='data points without the noise (our goal)')\n",
        "plt.plot(x_points, y_points, 'x', label='points in data (with noise)')\n",
        "plt.plot(x_points, y_pred, label='prediction')\n",
        "\n",
        "_, ylim_top = plt.ylim()\n",
        "plt.ylim(0, ylim_top+10)\n",
        "plt.xlabel('X-points (features)')\n",
        "plt.ylabel('Y-points (output / labels)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn03H6mTzqG5"
      },
      "outputs": [],
      "source": [
        "print(f'Expected coefficients/matrices:           {(w0, w1)}')\n",
        "\n",
        "# we have to do [0] on coef_ because coef_ itself is a matrix of all the coeffifients. In our case, coef_ just has one length but it's still an array.\n",
        "# [0] brings out the value from array which means better printing\n",
        "\n",
        "#############################\n",
        "# TODO, complete the statement\n",
        "print(f'Sklearn calculated coefficients/matrices: {(model.intercept_, ??)}')\n",
        "#############################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba-4Ox1r7bN4"
      },
      "source": [
        "## Let's step up the game with more complex functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYpUaxrn1VB-"
      },
      "outputs": [],
      "source": [
        "noise_scale_factor = 1.5\n",
        "num_points = 100\n",
        "sin_wave_ub = 3  # upper bound of sin wave y output. negative of this value would also be lower bound. (negative value here would make this a cosine wave)\n",
        "\n",
        "#############################\n",
        "# TODO, make x range between -1 and 1\n",
        "x_points = ??\n",
        "#############################\n",
        "y_points_no_noise = sin_wave_ub * np.sin(x_points*math.pi)\n",
        "\n",
        "noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)\n",
        "y_points = y_points_no_noise + noise\n",
        "\n",
        "plt.plot(x_points, y_points_no_noise, label='No noise output')\n",
        "plt.plot(x_points, y_points, label='Noisy output (labels fed to model)')\n",
        "\n",
        "plt.xlabel('X-points (features)')\n",
        "plt.ylabel('Y-points (output / labels)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ut2OpDhZTFX"
      },
      "source": [
        "#### Custom fitting with variable number of bases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTAmcROa-zg0"
      },
      "outputs": [],
      "source": [
        "# Custom calculating best fit\n",
        "\n",
        "\n",
        "min_num_bases = 1\n",
        "max_num_bases = 6\n",
        "\n",
        "\n",
        "assert min_num_bases <= max_num_bases, \"min_num_bases must be lesser or equal to max_num_bases. :|\"\n",
        "\n",
        "bases_preds = []\n",
        "\n",
        "for num_bases in range(min_num_bases, max_num_bases+1):\n",
        "  #############################\n",
        "  # TODO, implement feature engineering to find A\n",
        "  A = ??\n",
        "  #############################\n",
        "  A = np.array(A).T\n",
        "\n",
        "  # Solve for w to find our params\n",
        "  # Upper case: matrices. Lower case: vectors\n",
        "\n",
        "  #  Aw = y\n",
        "  #  Aᵀ(Aw) = Aᵀy  =>  AᵀAw = Aᵀy\n",
        "  #  w = (AᵀA)⁻¹Aᵀy\n",
        "\n",
        "  w = np.linalg.inv(A.T @ A) @ (A.T @ y_points)\n",
        "  #############################\n",
        "  #TODO, predict\n",
        "  y_pred = ??\n",
        "  #############################\n",
        "  bases_preds.append(y_pred)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(len(bases_preds), 1, figsize=(20, 10*len(bases_preds)))\n",
        "\n",
        "for i, (num_bases, preds) in enumerate(zip(range(min_num_bases, max_num_bases+1), bases_preds)):\n",
        "\n",
        "    axes[i].set_title(f'Num Bases = {num_bases}')\n",
        "\n",
        "    # Visualize the results\n",
        "    axes[i].plot(x_points, y_points_no_noise, label='data points without the noise (our goal)')\n",
        "    axes[i].plot(x_points, y_points, 'x',     label='points in data (with noise)')\n",
        "    axes[i].plot(x_points, bases_preds[i],    label='prediction')\n",
        "\n",
        "    axes[i].set_xlabel('X-points (features)')\n",
        "    axes[i].set_ylabel('Y-points (output / labels)')\n",
        "\n",
        "    axes[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV1p1u_CYPPL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}